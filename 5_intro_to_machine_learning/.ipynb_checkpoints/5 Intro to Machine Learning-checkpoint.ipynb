{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c56ae4",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Intro to Machine Learning\n",
    "\n",
    "\n",
    "*Covering:*\n",
    "* [Intro to Machine Learning Paradigm](#1)\n",
    "* [Machine Learning Workflow](#2)\n",
    "* [Evaluation Metrics](#3)\n",
    "* [Underfitting (Low Bias) and Overfitting (High Variance)](#4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3a298",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1. Intro to Machine Learning Paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9171ed7",
   "metadata": {},
   "source": [
    "Machine Learning is a new programming paradigm in which instead of explicitly programming computers to perform tasks, we let them learn from data in order to find the underlying patterns in the data. \n",
    "\n",
    "Here is an excerpt of ML definition from Wikipedia:\n",
    "\n",
    "***Machine learning (ML) is the study of computer algorithms that improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so. - Wikipedia.***\n",
    "\n",
    "\n",
    "What does that mean? \n",
    "\n",
    "Simple, we do not program machines. We show them the data and they figure out the rest. Take an example: if you wanted to build a program that can recognize human and horse with traditional programming, you would have to write every single instruction which differentiate horse from human, or set of codes that represent human/horse. But with Machine Learning, you can feed the image of the horse/humans in different scenarios, and the machine can figure out rules which can help us identify horse and human. \n",
    "\n",
    "This brings us to the next topic, Machine Learning Vs Traditional programming. \n",
    "\n",
    "\n",
    "### Machine Learning Vs Traditional programming\n",
    "\n",
    "The key inputs to any typical machine learning system are data and the answers (or labels). By providing those two things, we get the answers or rules. Let's think about it in terms of our previous example: if you provide a bunch of pictures of humans and horses to the ML system along with their labels(their names), you can get the rules which can help you perform the task of classifying horse and human. \n",
    "\n",
    "On the flip side, in standard programming, this is what we do and if you are a coder you know it well. In order to get results, you have to provide data and rules. \n",
    "\n",
    "### Types of Machine Learning\n",
    "\n",
    "In broad, there are 3 types of Machine learning:\n",
    "* Supervised learning where you have the data and the labels. Think of labels as what represents the input data. \n",
    "* Unsupervised learning: Where you do not have the labels\n",
    "* Reinforcement learning in which the goal is to optimize the rewards. Reinforcement learning has got its application in areas such as robotics. \n",
    "\n",
    "Most ML problems fall in the category of supervised learning. Let's take an example. In our example earlier, if horse images are labelled as horses and same for humans, it's easy for a machine to relate each image with its associated label. \n",
    "\n",
    "An example of unsupervised learning is a customer segmentation. Let's say that you want to provide promotions to a group of your clients based on their purchasing history, but you don't know these groups and their interests well. You just only have data. Using unsupervised techniques such as clustering, you can group customers who share the same interests and will likely all appreciate the promotion that you're offering. That's just one example, there are more applications of unsupervised learning. \n",
    "\n",
    "### Categories of ML Problems\n",
    "\n",
    "In terms of problems that we can solve with machine learning, there are 3 categories which are closely connected to the types we saw in the last section. \n",
    "\n",
    "* Classification: This falls in the supervised learning type. As we saw, the example can be to classify a horse or human. Whether you have two categories or more, they are all classification problems. With two categories, it is usually termed as binary classification, and multi-classification for more than two categories. \n",
    "\n",
    "* Regression: This is also a supervised type. The goal here is to predict a continuous value. Example is predicting the price of a house given its size, region, number of rooms, etc...\n",
    "\n",
    "* Clustering where the goal is to group some entities based on given characteristics. An example can be to group the customers based on some similar characteristics. \n",
    "\n",
    "### ML Applications\n",
    "\n",
    "Nowadays, most tech products/services possess some sort of machine learning algorithm running in the background, from browsers to our mobile phone. It's fun, my mobile is able to recognize me in any photo that I am part of. Or things like character recognition where your phone camera can read characters. \n",
    "\n",
    "It's fair to say that Machine Learning has transformed many industries, from banking, healthcare, production, streaming, to autonomous vehicles. Here are other detailed scenarios highlighting the applications of machine learning in the real world settings:\n",
    "\n",
    "* A bank or any credit card provider can detect fraud in real-time. Banks can also predict if a given customer requesting a loan will pay it back or not based on their financial history.\n",
    "* A Medical Analyst can diagnose a disease in a handful of minutes, or predict the likelihood or course of diseases or survival rate(Prognosis).\n",
    "* An engineer in a given industry can detect failure or defect on the equipment.\n",
    "* A telecommunication company can learn that a given customer is not satisfied with the service and is likely to opt-out from the service (churn).\n",
    "* Our email inboxes are smart enough to differentiate spams and important emails.\n",
    "* A given ads agency can place the relevant ads on a website that are likely to attract visitors. \n",
    "* A driverless car can confidently know that an object in front is a pedestrian.\n",
    "* A streaming service can suggest the best media to their clients based on their interests. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec0c4d",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1436d6",
   "metadata": {},
   "source": [
    "Different from standard programming, ML is made of code and data. Some steps in ML involve data while a small part involve code(ML algorithm). \n",
    "\n",
    "Here is an end to end ML workflow:\n",
    "\n",
    "* Defining the problem\n",
    "* Collecting the data\n",
    "* Establishing the baseline\n",
    "* Exploratory Data Analysis\n",
    "* Feature Engineering\n",
    "* Choosing/Creating/Training a ML model\n",
    "* Performing Error analysis\n",
    "* Deploying a model\n",
    "\n",
    "Let's elaborate it more, step by step. \n",
    "\n",
    "### 1. Problem definition\n",
    "\n",
    "Problem definition is the important and the initial step in any ML project. This is where you make sure you understand the problem really well. Understanding the problem will give you proper intuitions about the next steps to follow such as right learning algorithms, etc.\n",
    "\n",
    "### 2. Collecting the data\n",
    "\n",
    "After you have defined your problem, the next step is to find the relevant data. \n",
    "Nowadays, there are many open source datasets, be it images, texts, or structured data (data in tabular fashion) on platforms like Kaggle, Google Datasets, UCL, and some government websites. Your job as ML Engineer is to find the relevant data that you can use to solve a presented problem. \n",
    "\n",
    "But there are times that you will have to collect your dataset, especially if you are solving a problem that no one solved before. In this case, consider the time that you will have to spend collecting data and the cost. You also do not need to wait until you have your desired data points before you can start. Embrace ML development early on so that you can learn if you (really) need more data or improvements you can make for further collections. This idea is inspired by Andrew Ng. \n",
    "\n",
    "Also, when collecting the data, quality is better than quantity. There are times where small data but good data can outwork big poor data. \n",
    "The amount of data you need is going to depend on the problem you're solving and its scope, but whatever the problem is, aiming to collect good data is the way to go. \n",
    "\n",
    "### 3. Establishing a baseline\n",
    "\n",
    "Without a benchmark, you won't know how to evaluate your results properly. \n",
    "A baseline is the simplest model that can solve your project. It does not have to be a model. It can be an open source application, a statistical analysis or intuitions you get from data at a quick glance. \n",
    "\n",
    "Take an example: Let's say that you want to build a cat and dog classifier and you have 2000 images, where 1400 are cats and 600 are dogs. Before you build a model there is already a 70% chance that any random image you pick up will be a cat. In this case, 70% is your simple baseline and it means your goal is to predict cats with 70% accuracy or more. \n",
    "If you can't beat the baseline, sometimes it means the project is not worth pursuing. \n",
    "\n",
    "### 4. Exploratory Data Analysis(EDA)\n",
    "\n",
    "Before manipulating the data, it is quite important to go through it with the goal of learning the dataset. This can be overlooked, but doing it well will help you to know the effective strategies to be applied while cleaning the data. \n",
    "\n",
    "Go through some values, plot some features, and try to understand the correlation between them. If the work is vision-related, visualize some images, spot what’s missing in the images. Are they diverse enough? What types of image scenarios can you add? Or what are images that can mislead the model?\n",
    "\n",
    "Here are other things that you would want to inspect in the data:\n",
    "* Duplicate values\n",
    "* Corrupted or data with unsupported formats (ex: having an image of .txt and with 0 Kilobytes)\n",
    "* Class imbalances\n",
    "* Biases that can be present in the data\n",
    "\n",
    "Before performing EDA, split the data into the training set, validation and test sets to avoid data leakage. \n",
    "\n",
    "### 5. Data Processing\n",
    "\n",
    "Data processing is perhaps the biggest part of an ML project. There is a notion that Data Scientists and Machine Learning Engineers spend more than 80% preparing the data and this makes sense, the real world datasets are messy. \n",
    "\n",
    "In this step, it is where you convert the raw data to go in a format that can be accepted by the ML algorithm. It can mean manipulating features to be in their proper formats or create new features from the existing ones. \n",
    "\n",
    "Feature engineering being a part of data processing is also where things get creative. In terms of structured data, the way you engineer a numerical feature is going to be different to how you engineer the categorical features. Also in unstructured data, the way you manipulate images is going to be different to how you manipulate texts or sounds. \n",
    "\n",
    "As the next parts will cover the practical implementations of this step in various data types (tabular, images, texts), let's be general about things you're likely going to deal with while manipulating the features:\n",
    "\n",
    "* **Imputing missing values**: Missing values can either be filled, removed or left as they are. There are various strategies for missing values such as mean, median, frequent imputations, backward and forward fill, and iterative imputations. The right imputation technique depends on the problem and the dataset.  While imputing missing values is appreciated for most machine learning algorithms, tree models can not suffer from them. So if you are using models like Random Forest or Decision tree, you can leave missing values as they are. \n",
    "\n",
    "* **Encoding categorical features**: Categorical features are all types of features that have categorical values. For example, A gender feature having the values Male and Female is a categorical feature. You will want to encode such types of features. The techniques for encoding them are label encoding where you can assign 1 to Male, and 2 to Female, or one hot encoding where you can get the binary representations (0s and 1s) in one hot matrix. You will see this in practice. \n",
    "\n",
    "* **Normalizing/Standardizing the numeric features**: Most ML models will work well when the input values are scaled to small values and that will result in training fast as well as converging fast. Normalization will set the values to between 0 and 1 whereas Standardization rescale the features to have mean of 0 and unit standard deviation. If you know that your data has normal or gausian distribution, normalization can be a good choice. Otherwise, Standardization will work well by default. \n",
    "\n",
    "* **Dealing with date features**: The date in the format we know it may not be recognized by the model. If we have Day-MM--YY, we can create a day, month and column features. \n",
    "\n",
    "Data preparation is a huge work and it depends on the dataset you are working with. The type of feature processing that you can apply is unique and will depend on the problem at hand and the available dataset. \n",
    "\n",
    "### 6. Choosing/Creating a ML model\n",
    "\n",
    "Choosing and Creating a model is the tiniest part in a typical machine learning workflow. \n",
    "There are different types of models but most of them will fall in these categories: linear models such as linear/logistic regression, tree based models like decision trees, ensembles such as Gradient Boost or XGBoost, and neural networks. \n",
    "\n",
    "All of these are what you can choose from when creating a model for your problem. Getting a model that can ultimately solve your problem is no free lunch scenario. You will have to experiment with different models and tune hyper-parameters, things like that. \n",
    "\n",
    "To reduce your modelling curve, here are a few things that you can consider while choosing a machine learning model. \n",
    "\n",
    "* **The scope of the problem**: There are a set of problems which directly give a clue of the right learning algorithm. Take an example: If you are going to build an image classifier from big data, neural networks (Convolutional neural networks specifically) might be a go to algorithm. \n",
    "\n",
    "* **The size of the dataset**: Linear models tend to work well in small data problems whereas ensembles and neural networks can work well in complex problems. \n",
    "\n",
    "* **The level of interpretability**: If you want the results of your model to be explainable, neural networks will not be a good choice. Tree based models such as decision trees can be explainable compared to other models. \n",
    "\n",
    "* **Training time**: Complex models (neural nets and ensembles) will take too long to train and thus exhausting the computation resources. On the other hand, linear models can train faster. \n",
    "\n",
    "So as you can see, there is a trade off. You want explainability, choose models which can provide that for you. You have a small dataset or you care about the training time, same thing, choose a reasonable algorithm.\n",
    "\n",
    "It is also worth mentioning again that choosing and training a model is an iterative process. With the ML frameworks available such as Scikit-Learn, TensorFlow or PyTorch, building is easy. But getting the model to converge is another thing. It is often data improvement aided by error analysis that will ultimately improve the model. \n",
    "\n",
    "### 7. Performing Error Analysis\n",
    "\n",
    "Performing the error analysis will guide you throughout the data and the model improvement. In order to spot the errors, we have to iteratively ask the right questions. \n",
    "\n",
    "Because a good model comes from good data, we will want to keep the mode fixed to see the types of errors that are happening. Here are questions that we can ask ourselves:\n",
    "\n",
    "* Is the model doing poorly on all classes or is it one specific class?\n",
    "* Is it because there are not enough data points for that particular class compared to other classes?\n",
    "* There are trade-offs and limits on how much you can do to reduce the error. Is there room for improvement?\n",
    "\n",
    "Often, the improvement will not come from tuning the model, but spending time to increase the data quality. \n",
    "\n",
    "When improving the data, you can create artificial data (a.ka data augmentation). This will work well most of the time. As this step is iterative too, keep doing error analysis and continuously aim to improve the data. \n",
    "\n",
    "### 8. Deploying, Monitoring and Maintaining the Model\n",
    "\n",
    "Model deployment is the last part in this workflow. When everything has gone right, your model converged or you came to find the reliable data, the next step will be to deploy the model so that the users can start to make requests and get predictions or enhanced services (ML in action).  \n",
    "\n",
    "Model deployment is not in the scope of this introduction. If you want to learn more about it, I recommend Machine Learning Engineering for Production (MLOps) Specialization - Deeplearning.AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd83957",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3969c6c",
   "metadata": {},
   "source": [
    "Earlier in this introduction to Machine Learning, we saw that most problems are either regression or classification. In this section, we will learn the evaluation metrics that are used in evaluating the performance of the machine learning models. \n",
    "\n",
    "Let's kick this off with the regression metrics!\n",
    "\n",
    "### Regression Metrics\n",
    "\n",
    "In regression, the goal is to predict the continuous value. The difference between the actual value and the predicted value is called the error.\n",
    "\n",
    "*Error = Actual value - Predicted value*\n",
    "\n",
    "We can represent the error above in squares, hence Mean Squared Error (MSE). \n",
    "\n",
    "*MSE = SQUAREROOT(Actual value - Predicted value)*\n",
    "\n",
    "Taking the square root of the mean squared error will give the Root Mean Squared Error(RMSE). RMSE is the most used regression metric. \n",
    "\n",
    "There are times that you will work with the datasets containing outliers. In this case, the commonly used metric is called Mean Absolute Error (MAE). As simple as calculating MSE, MAE is also the absolute of the error. \n",
    "\n",
    "*MAE = ABSOLUTE (Actual value - Predicted Value)*\n",
    "\n",
    "Like said, MAE is very sensitive to outliers. This will make it a suitable metric for all kinds of problems which are likely to have abnormal scenarios such as time series. \n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "In classification problems, the goal is to predict the categories/class. Accuracy is the most used metric. The accuracy shows the ability of the model in making the correct predictions. Take an example, in a horse/human classifier. If you have 250 training images for horses and the same number for humans, and the model can confidently predict 400 images, then the accuracy is 400/500 = 0.8, so your model is 80% accurate. \n",
    "\n",
    "The accuracy is simply an indicator of how your model is in making correct predictions and it will only be useful if you have a balanced dataset (like we had 250 images for horses and 250 images for humans). \n",
    "\n",
    "When we have a skewed dataset or when there are imbalances, we need a different perspective on how we evaluate the model. Take an example, if we have 450 images for horses and 50 images for humans, there is a chance of 90% (450/500) that the horse will be correctly predicted, because the dataset is dominated by the horses. But how about humans? Well, it's obvious that the model will struggle predicting them correctly. \n",
    "\n",
    "This is where we introduce other metrics that can be far more useful than accuracy, such as precision, recall, and F1 score. \n",
    "\n",
    "#### Precision and Recall, and F1 Score\n",
    "\n",
    "Before we talk about precision and recall, let’s first revise their components:\n",
    "\n",
    "* True Positive (TP): These are the examples that model classified as positive, when their actual labels are positive\n",
    "* False Positive(FP): These are examples that the model classified as positive, when in fact their labels are negative\n",
    "* True Negative (TN): These are the examples that the model classified as negative, and their actual labels are negative \n",
    "* False Negative (FN): These are the examples that the model classified as negative, when in fact, their actual labels are positive. \n",
    "\n",
    "The accuracy that we talked about is the number of correct examples over total examples. So, that is \n",
    "\n",
    "*Accuracy = (TP + TN) / (TP+TN+FP+FN)*\n",
    "\n",
    "Precision is the model accuracy on predicting positive examples. \n",
    "\n",
    "*Precision = TP / (TP + FP)*\n",
    "\n",
    "On the other hand, Recall is the model ability to predict the positive examples correctly. \n",
    "\n",
    "*Recall = TP / (TP+FN)*\n",
    "\n",
    "The higher the recall and precision, the better the model is at making accurate predictions but there is a tradeoff between them. Increasing precision will reduce the recall and vice versa. \n",
    "\n",
    "We can combine both precision and recall to get another metric called F1 Score. F1 Score is the harmonic mean of precision and recall. \n",
    "\n",
    "*F1 Score = 2 *(precision * recall) / (precision + recall)*\n",
    "\n",
    "You may have wondered why I didn’t include a confusion matrix(CM) in this section. CM is not a metric, it is a matrix that we can use to easily compare the predictions and actual classes. \n",
    "\n",
    "Both accuracy, CM, precision, recall, and F1 score are implemented easily in Scikit-Learn, a Machine Learning framework used to build classical ML algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452fbfb",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4. Underfitting and Overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478654a6",
   "metadata": {},
   "source": [
    "Building a machine learning model that can fit the training data well is not a trivial task. Often, at the initial training, the model will either underfit or overfit the data. Some machine learning models proves that really well. Take an example: When training a decision trees, it is very likely that they will overfit the data at first.\n",
    "\n",
    "There is a trade off between underfitting/overfitting, and so it's important to understand the difference between them and how to handle each and each. Understanding and handling underfitting/overfitting is a critical task in diagonizing machine learning models. \n",
    "\n",
    "### Underfitting (Low Bias)\n",
    "\n",
    "Underfitting happens when the model does poor on the training data. It can be caused by the fact that the model is simple for the training data or the data is not good enough for what you are trying to predict. \n",
    "\n",
    "\n",
    "Here are some of the techniques that can be used to deal with a model which has low bias(underfit): \n",
    "\n",
    "* Use complex models. If you are using linear models, try other complex models like Random forests or Support Vector Machines. Not to mention neural networks if you are dealing with unstructured data (images, texts,..)\n",
    "* Add more training data and use good features. Good features have high predictive power. \n",
    "* Reduce the regularization.\n",
    "* If you're using neural networks, increase the number of epochs/training iterations. If the epochs are very low, the model may not be able to learn the underlying rules in data and so it will not perform well. \n",
    "\n",
    "\n",
    "### Overfitting (High Variance)\n",
    "\n",
    "Overfitting is the reverse of underfitting. An overfitted model will do well on the training data but will be so poor when supplied with a new data (the data that the model never saw). \n",
    "\n",
    "Overfitting is caused by using model which is too complex for the dataset and few training examples. \n",
    "\n",
    "Here are techniques to handle overfitting:\n",
    "\n",
    "* Try simple models or simplify the current model. Some machine learning algorithms can be simplified. Take an example: in neural networks, you can reduce the number of layers or neurons. Also in classical algorithms like Support Vector Machines, you can try different kernels, a linear kernel is simple than a polynomial kernel. \n",
    "* Find more training data. \n",
    "* Regularize the model by tuning hyperparameters.\n",
    "\n",
    "\n",
    "To summarize this, it is very important to be able to understand why the model is not doing well. If the model is being poor on the data it was trained on, you know it is underfitting and you know what to do about it. \n",
    "\n",
    "Also with the exception of improving/increasing the training data, often you have to tune hyperparameters to get a model that can generalize well. While there are techniques that simplified hyperparameter search(like Grid search, Random search, Keras tuner), it is important to understand the hyperparameters of the model you are using so that you can know their proper values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1231a33",
   "metadata": {},
   "source": [
    "### [BACK TO TOP](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096c062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tensor': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd034ac5db714c5906ee087fcf6e2d00ee4febf096586592b6ba3662ed3b7e7a5f6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
